{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call([\"pip\", \"install\", \"nltk\"])\n",
    "call([\"pip\", \"install\", \"textblob\"])\n",
    "call([\"pip\", \"install\", \"IDisplay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape swear word list\n",
    "\n",
    "We scrape swear words from the web from the site:\n",
    "http://www.noswearing.com/\n",
    "\n",
    "It is a community driven list of swear words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niggas', 'anus', 'arse', 'arsehole', 'ass', 'ass-hat', 'ass-jabber', 'ass-pirate', 'assbag', 'assbandit', 'assbanger', 'assbite', 'assclown', 'asscock', 'asscracker', 'asses', 'assface', 'assfuck', 'assfucker', 'assgoblin', 'asshat', 'asshead', 'asshole', 'asshopper', 'assjacker', 'asslick', 'asslicker', 'assmonkey', 'assmunch', 'assmuncher', 'assnigger', 'asspirate', 'assshit', 'assshole', 'asssucker', 'asswad', 'asswipe', 'axwound', 'bampot', 'bastard', 'beaner', 'bitch', 'bitchass', 'bitches', 'bitchtits', 'bitchy', 'blow job', 'blowjob', 'bollocks', 'bollox', 'boner', 'brotherfucker', 'bullshit', 'bumblefuck', 'butt plug', 'butt-pirate', 'buttfucka', 'buttfucker', 'camel toe', 'carpetmuncher', 'chesticle', 'chinc', 'chink', 'choad', 'chode', 'clit', 'clitface', 'clitfuck', 'clusterfuck', 'cock', 'cockass', 'cockbite', 'cockburger', 'cockface', 'cockfucker', 'cockhead', 'cockjockey', 'cockknoker', 'cockmaster', 'cockmongler', 'cockmongruel', 'cockmonkey', 'cockmuncher', 'cocknose', 'cocknugget', 'cockshit', 'cocksmith', 'cocksmoke', 'cocksmoker', 'cocksniffer', 'cocksucker', 'cockwaffle', 'coochie', 'coochy', 'coon', 'cooter', 'cracker', 'cum', 'cumbubble', 'cumdumpster', 'cumguzzler', 'cumjockey', 'cumslut', 'cumtart', 'cunnie', 'cunnilingus', 'cunt', 'cuntass', 'cuntface', 'cunthole', 'cuntlicker', 'cuntrag', 'cuntslut', 'dago', 'damn', 'deggo', 'dick', 'dick-sneeze', 'dickbag', 'dickbeaters', 'dickface', 'dickfuck', 'dickfucker', 'dickhead', 'dickhole', 'dickjuice', 'dickmilk', 'dickmonger', 'dicks', 'dickslap', 'dicksucker', 'dicksucking', 'dicktickler', 'dickwad', 'dickweasel', 'dickweed', 'dickwod', 'dike', 'dildo', 'dipshit', 'doochbag', 'dookie', 'douche', 'douche-fag', 'douchebag', 'douchewaffle', 'dumass', 'dumb ass', 'dumbass', 'dumbfuck', 'dumbshit', 'dumshit', 'dyke', 'fag', 'fagbag', 'fagfucker', 'faggit', 'faggot', 'faggotcock', 'fagtard', 'fatass', 'fellatio', 'feltch', 'flamer', 'fuck', 'fuckass', 'fuckbag', 'fuckboy', 'fuckbrain', 'fuckbutt', 'fuckbutter', 'fucked', 'fucker', 'fuckersucker', 'fuckface', 'fuckhead', 'fuckhole', 'fuckin', 'fucking', 'fucknut', 'fucknutt', 'fuckoff', 'fucks', 'fuckstick', 'fucktard', 'fucktart', 'fuckup', 'fuckwad', 'fuckwit', 'fuckwitt', 'fudgepacker', 'gay', 'gayass', 'gaybob', 'gaydo', 'gayfuck', 'gayfuckist', 'gaylord', 'gaytard', 'gaywad', 'goddamn', 'goddamnit', 'gooch', 'gook', 'gringo', 'guido', 'handjob', 'hard on', 'heeb', 'hell', 'ho', 'hoe', 'homo', 'homodumbshit', 'honkey', 'humping', 'jackass', 'jagoff', 'jap', 'jerk off', 'jerkass', 'jigaboo', 'jizz', 'jungle bunny', 'junglebunny', 'kike', 'kooch', 'kootch', 'kraut', 'kunt', 'kyke', 'lameass', 'lardass', 'lesbian', 'lesbo', 'lezzie', 'mcfagget', 'mick', 'minge', 'mothafucka', \"mothafuckin\\\\'\", 'motherfucker', 'motherfucking', 'muff', 'muffdiver', 'munging', 'negro', 'nigaboo', 'nigga', 'nigger', 'niggers', 'niglet', 'nut sack', 'nutsack', 'paki', 'panooch', 'pecker', 'peckerhead', 'penis', 'penisbanger', 'penisfucker', 'penispuffer', 'piss', 'pissed', 'pissed off', 'pissflaps', 'polesmoker', 'pollock', 'poon', 'poonani', 'poonany', 'poontang', 'porch monkey', 'porchmonkey', 'prick', 'punanny', 'punta', 'pussies', 'pussy', 'pussylicking', 'puto', 'queef', 'queer', 'queerbait', 'queerhole', 'renob', 'rimjob', 'ruski', 'sand nigger', 'sandnigger', 'schlong', 'scrote', 'shit', 'shitass', 'shitbag', 'shitbagger', 'shitbrains', 'shitbreath', 'shitcanned', 'shitcunt', 'shitdick', 'shitface', 'shitfaced', 'shithead', 'shithole', 'shithouse', 'shitspitter', 'shitstain', 'shitter', 'shittiest', 'shitting', 'shitty', 'shiz', 'shiznit', 'skank', 'skeet', 'skullfuck', 'slut', 'slutbag', 'smeg', 'snatch', 'spic', 'spick', 'splooge', 'spook', 'suckass', 'tard', 'testicle', 'thundercunt', 'tit', 'titfuck', 'tits', 'tittyfuck', 'twat', 'twatlips', 'twats', 'twatwaffle', 'unclefucker', 'va-j-j', 'vag', 'vagina', 'vajayjay', 'vjayjay', 'wank', 'wankjob', 'wetback', 'whore', 'whorebag', 'whoreface', 'wop']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from lxml import html\n",
    "\n",
    "def requests_get(url):        \n",
    "    ua = UserAgent().random  \n",
    "    return requests.get(url, headers={'User-Agent': ua})\n",
    "\n",
    "def get_swear_words(save_file='swear_words.txt'): \n",
    "    \"\"\"\n",
    "    Scrapes a comprehensive list of swear words from noswearing.com\n",
    "    \"\"\"\n",
    "    words = ['niggas']\n",
    "    if os.path.isfile(save_file):\n",
    "        with open(save_file, 'rt') as f:\n",
    "            for line in f:\n",
    "                words.append(line.strip())\n",
    "        \n",
    "        return words\n",
    "        \n",
    "    base_url = 'http://www.noswearing.com/dictionary/'\n",
    "    letters = '1' + string.ascii_lowercase\n",
    "    \n",
    "    for letter in letters:\n",
    "        full_url = base_url + letter\n",
    "        result = requests_get(full_url)\n",
    "        tree = html.fromstring(result.text)\n",
    "        search = tree.xpath(\"//td[@valign='top']/a[@name and string-length(@name) != 0]\")\n",
    "        \n",
    "        if search is None:\n",
    "            continue\n",
    "        \n",
    "        for result in search:\n",
    "            words.append(result.get('name').lower())\n",
    "            \n",
    "    with open(save_file, 'wt') as f:\n",
    "        for word in words:\n",
    "            f.write(word)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    return words\n",
    "\n",
    "print(get_swear_words())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing TextBlob\n",
    "\n",
    "I don't really like TextBlob as it tries to be \"nice\", but lacks a lot of basic functionality.\n",
    "\n",
    "1. Stop words not included\n",
    "2. Tokenizer is pretty meh.\n",
    "3. No built in way to obtain word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "from textblob import TextBlob, WordList\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_data_paths():\n",
    "    dir_path = os.path.dirname(os.path.realpath('.'))\n",
    "    data_dir = os.path.join(dir_path, 'billboard-hot-100-data')\n",
    "    dirs = [os.path.join(data_dir, d, 'songs.csv') for d in os.listdir(data_dir) \n",
    "            if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "def lyric_file_to_text_blob(row):\n",
    "    \"\"\"\n",
    "    Appends lyrics column to data frame given that it has the 'lyrics_path' column.\n",
    "    \"\"\"\n",
    "    text_blob = None\n",
    "    \n",
    "    with open(row['lyrics_path'], 'r') as f:\n",
    "        text_blob = TextBlob(f.read())\n",
    "        \n",
    "    return text_blob\n",
    "\n",
    "def remove_stop_words(word_list):\n",
    "    wl = WordList([])\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    for word in word_list:\n",
    "        if word.lower() not in stop_words:\n",
    "            wl.append(word)\n",
    "    \n",
    "    return wl\n",
    "\n",
    "def word_freq(words, sort='desc'):\n",
    "    \"\"\"\n",
    "    Returns frequency table for all words provided in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    reverse = sort == 'desc'\n",
    "    \n",
    "    freq = {}\n",
    "    for word in words:\n",
    "        if word in freq:\n",
    "            freq[word] = freq[word] + 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "            \n",
    "    return sorted(freq.items(), key=operator.itemgetter(1), reverse=reverse)\n",
    "\n",
    "data_paths = get_data_paths()\n",
    "songs = pd.read_csv(data_paths[0])\n",
    "songs[\"lyrics\"] = songs.apply(lyric_file_to_text_blob, axis=1)\n",
    "\n",
    "all_words = WordList([])\n",
    "\n",
    "for i, row in songs.iterrows():\n",
    "    all_words.extend(row['lyrics'].words)\n",
    "\n",
    "cleaned_all_words = remove_stop_words(all_words)\n",
    "cleaned_all_words = pd.DataFrame(word_freq(cleaned_all_words.lower()), columns=['word', 'frequency'])\n",
    "cleaned_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def lyric_file_to_text(row):\n",
    "    \"\"\"\n",
    "    Adds lyrics to given panda data frame as a string. Must have lyrics_path.\n",
    "    \"\"\"\n",
    "    text = None\n",
    "    \n",
    "    with open(row['lyrics_path'], 'r') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text\n",
    "\n",
    "def remove_extra_junk(word_list):\n",
    "    words = []\n",
    "    remove = [\",\", \"n't\", \"'m\", \")\", \"(\", \"'s\", \"'\", \"]\", \"[\"]\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word not in remove:\n",
    "            words.append(word)\n",
    "            \n",
    "    return words\n",
    "    \n",
    "    \n",
    "data_paths = get_data_paths()\n",
    "songs = pd.read_csv(data_paths[0])\n",
    "songs[\"lyrics\"] = songs.apply(lyric_file_to_text, axis=1)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for i, row in songs.iterrows():\n",
    "    all_words.extend(nltk.tokenize.word_tokenize(row['lyrics']))\n",
    "\n",
    "cleaned_all_words = [w.lower() for w in remove_extra_junk(remove_stop_words(all_words))]\n",
    "freq_dist = nltk.FreqDist(cleaned_all_words)\n",
    "\n",
    "freq_dist.plot(50)\n",
    "freq_dist.most_common(100)\n",
    "#cleaned_all_words = pd.DataFrame(word_freq(cleaned_all_words), columns=['word', 'frequency'])\n",
    "#cleaned_all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetitive songs skewing data?\n",
    "\n",
    "Some songs may be super reptitive. Lets look at a couple of songs that have the word in the title. These songs probably repeat the title a decent amount in their song. Hence treating all lyrics as one group of text less reliable in analyzing frequency.\n",
    "\n",
    "To simplify this process, we can look at only single word titles. This will at least give us a general idea if the data could be skewed by a single song or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, song in songs.iterrows():\n",
    "    title = song['title']\n",
    "    title_words = title.split(' ')\n",
    "    \n",
    "    if len(title_words) > 1:\n",
    "        continue\n",
    "    \n",
    "    lyrics = song['lyrics']\n",
    "    words = nltk.tokenize.word_tokenize(lyrics)\n",
    "    clean_words = [w.lower() for w in remove_extra_junk(remove_stop_words(words))]\n",
    "    \n",
    "    dist = nltk.FreqDist(clean_words)\n",
    "    freq = dist.freq(title_words[0].lower())\n",
    "    \n",
    "    if freq > .1:\n",
    "        print(song['artist'], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seems pretty reptitive\n",
    "\n",
    "There are a handful of single word song titles that repeat the title within the song at least 10% of the time. This gives us a general idea that there is most likely a skew to the data. I think it is safe to assume that if a single word is repeated many times, the song is most likely reptitive.\n",
    "\n",
    "Lets look at the song \"water\" by Ugly God to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_title_to_analyze = 'Water'\n",
    "\n",
    "lyrics = songs['lyrics'].where(songs['title'] == song_title_to_analyze, '').max()\n",
    "print(lyrics)\n",
    "words = nltk.tokenize.word_tokenize(lyrics)\n",
    "clean_words = [w.lower() for w in remove_extra_junk(remove_stop_words(words))]\n",
    "water_dist = nltk.FreqDist(clean_words)\n",
    "water_dist.plot(25)\n",
    "\n",
    "water_dist.freq(song_title_to_analyze.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at swear word distribution\n",
    "\n",
    "Let's look at the distribution of swear words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sws = []\n",
    "\n",
    "for sw in set(get_swear_words()):\n",
    "    sws.append({'word': sw,\n",
    "                'dist': freq_dist.freq(sw)})\n",
    "    \n",
    "sw_df = pd.DataFrame.from_dict(sws)\n",
    "sw_df.nlargest(10, 'dist').plot(x='word', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
